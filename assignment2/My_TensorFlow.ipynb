{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses_epoch = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            losses_epoch.append(loss*actual_batch_size)\n",
    "            \n",
    "            '''\n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            '''          \n",
    "                \n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses_epoch)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Первоначальная модель\n",
    "# Feel free to play with this cell\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[7, 7, 3, 32])\n",
    "    b1 = tf.get_variable('b1', shape=[32])\n",
    "    W2 = tf.get_variable('W2', shape=[5408, 1024])\n",
    "    b2 = tf.get_variable('b2', shape=[1024])\n",
    "    W3 = tf.get_variable('W3', shape=[1024, 10])\n",
    "    b3 = tf.get_variable('b3', shape=[10])\n",
    "    \n",
    "    conv  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv)\n",
    "    bn = tf.layers.batch_normalization(relu1, training=is_training)\n",
    "    pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    pool_flat = tf.reshape(pool, [-1,5408])\n",
    "    af1 = tf.matmul(pool_flat, W2) + b2\n",
    "    relu2 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu2, W3) + b3\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 1e-2\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 conv layers\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 1024])\n",
    "    b3 = tf.get_variable('b3', shape=[1024])\n",
    "    W4 = tf.get_variable('W4', shape=[1024, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    #bn = tf.layers.batch_normalization(relu1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(pool1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    pool_flat = tf.reshape(pool2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 1e-2\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 conv layers + bn\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 1024])\n",
    "    b3 = tf.get_variable('b3', shape=[1024])\n",
    "    W4 = tf.get_variable('W4', shape=[1024, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 conv layers + bn + 2fc\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 1024])\n",
    "    b3 = tf.get_variable('b3', shape=[1024])\n",
    "    \n",
    "    W4 = tf.get_variable('W4', shape=[1024, 1024])\n",
    "    b4 = tf.get_variable('b4', shape=[1024])\n",
    "    W5 = tf.get_variable('W5', shape=[1024, 1024])\n",
    "    b5 = tf.get_variable('b5', shape=[1024])\n",
    "    W6 = tf.get_variable('W6', shape=[1024, 10])\n",
    "    b6 = tf.get_variable('b6', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    \n",
    "    relu4 = tf.nn.relu(af2)\n",
    "    af3 = tf.matmul(relu4, W5) + b5\n",
    "    relu5 = tf.nn.relu(af3)\n",
    "    af4 = tf.matmul(relu5, W6) + b6\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_out = af4\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 conv layers + bn + 1024->2048\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 2048])\n",
    "    b3 = tf.get_variable('b3', shape=[2048])\n",
    "    W4 = tf.get_variable('W4', shape=[2048, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-9d760fbc2567>:54: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 conv layers + bn + 1024->2048 + change H_f\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 3\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 2\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 2048])\n",
    "    b3 = tf.get_variable('b3', shape=[2048])\n",
    "    W4 = tf.get_variable('W4', shape=[2048, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 1.24 and accuracy of 0.585\n",
      "Epoch 2, Overall loss = 0.658 and accuracy of 0.776\n",
      "Epoch 3, Overall loss = 0.379 and accuracy of 0.881\n",
      "Epoch 4, Overall loss = 0.214 and accuracy of 0.934\n",
      "Epoch 5, Overall loss = 0.181 and accuracy of 0.94\n",
      "Epoch 6, Overall loss = 0.135 and accuracy of 0.955\n",
      "Epoch 7, Overall loss = 0.0869 and accuracy of 0.972\n",
      "Epoch 8, Overall loss = 0.0667 and accuracy of 0.978\n",
      "Epoch 9, Overall loss = 0.064 and accuracy of 0.979\n",
      "Epoch 10, Overall loss = 0.0607 and accuracy of 0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niDHGmJBYAjHGGBOS/wd9QD7WH4wUawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c622f41668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 0.098 and accuracy of 0.968\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.67 and accuracy of 0.701\n",
      "Wall time: 47.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,10,64,100,train_step,True)\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на трейне и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 0.098 and accuracy of 0.968\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.67 and accuracy of 0.701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.674767809867859, 0.701)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 70% accuracy on Validation\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test Set - Do this only once\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Epoch 1, Overall loss = 1.72 and accuracy of 0.704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.7242294297218324, 0.704)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
