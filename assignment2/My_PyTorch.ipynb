{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import timeit\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,\n",
    "                          transform=T.ToTensor())\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "        \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для CPU\n",
    "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(dtype))\n",
    "            y_var = Variable(y.type(dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(5408, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 10), # affine layer\n",
    "              )\n",
    "\n",
    "model = model_base.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель из Tensorflow\n",
    "F1 = 32\n",
    "H_f1 = 3\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 2\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "\n",
    "\n",
    "model_base = nn.Sequential(\n",
    "                nn.Conv2d(3, F1, kernel_size=H_f1, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(F1),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(F1, F2, kernel_size=H_f2, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(F2),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(w3_size0, 2048),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(2048, 10), # affine layer\n",
    "              )\n",
    "\n",
    "model = model_base.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель из Tensorflow\n",
    "# + dropout1\n",
    "F1 = 32\n",
    "H_f1 = 3\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 2\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "\n",
    "\n",
    "model_base = nn.Sequential(\n",
    "                nn.Conv2d(3, F1, kernel_size=H_f1, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(F1),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(F1, F2, kernel_size=H_f2, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(F2),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(w3_size0, 2048),\n",
    "                nn.Dropout(p=0.5, inplace=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(2048, 10), # affine layer\n",
    "              )\n",
    "\n",
    "model = model_base.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "t = 100, loss = 1.2378\n",
      "t = 200, loss = 1.3767\n",
      "t = 300, loss = 1.3721\n",
      "t = 400, loss = 0.9606\n",
      "t = 500, loss = 0.8954\n",
      "t = 600, loss = 1.1817\n",
      "t = 700, loss = 1.2300\n",
      "Starting epoch 2 / 10\n",
      "t = 100, loss = 0.7262\n",
      "t = 200, loss = 0.7422\n",
      "t = 300, loss = 0.8236\n",
      "t = 400, loss = 0.7510\n",
      "t = 500, loss = 0.5775\n",
      "t = 600, loss = 0.8112\n",
      "t = 700, loss = 0.7392\n",
      "Starting epoch 3 / 10\n",
      "t = 100, loss = 0.3580\n",
      "t = 200, loss = 0.4491\n",
      "t = 300, loss = 0.4597\n",
      "t = 400, loss = 0.5209\n",
      "t = 500, loss = 0.3419\n",
      "t = 600, loss = 0.4047\n",
      "t = 700, loss = 0.3734\n",
      "Starting epoch 4 / 10\n",
      "t = 100, loss = 0.2243\n",
      "t = 200, loss = 0.2113\n",
      "t = 300, loss = 0.1824\n",
      "t = 400, loss = 0.1202\n",
      "t = 500, loss = 0.1652\n",
      "t = 600, loss = 0.1459\n",
      "t = 700, loss = 0.0754\n",
      "Starting epoch 5 / 10\n",
      "t = 100, loss = 0.1815\n",
      "t = 200, loss = 0.0596\n",
      "t = 300, loss = 0.1606\n",
      "t = 400, loss = 0.0733\n",
      "t = 500, loss = 0.1033\n",
      "t = 600, loss = 0.0727\n",
      "t = 700, loss = 0.0705\n",
      "Starting epoch 6 / 10\n",
      "t = 100, loss = 0.1673\n",
      "t = 200, loss = 0.1129\n",
      "t = 300, loss = 0.0371\n",
      "t = 400, loss = 0.1864\n",
      "t = 500, loss = 0.0611\n",
      "t = 600, loss = 0.0266\n",
      "t = 700, loss = 0.1138\n",
      "Starting epoch 7 / 10\n",
      "t = 100, loss = 0.0626\n",
      "t = 200, loss = 0.0974\n",
      "t = 300, loss = 0.1026\n",
      "t = 400, loss = 0.0166\n",
      "t = 500, loss = 0.0455\n",
      "t = 600, loss = 0.0261\n",
      "t = 700, loss = 0.0373\n",
      "Starting epoch 8 / 10\n",
      "t = 100, loss = 0.0914\n",
      "t = 200, loss = 0.0174\n",
      "t = 300, loss = 0.0189\n",
      "t = 400, loss = 0.0980\n",
      "t = 500, loss = 0.0232\n",
      "t = 600, loss = 0.0419\n",
      "t = 700, loss = 0.0740\n",
      "Starting epoch 9 / 10\n",
      "t = 100, loss = 0.0238\n",
      "t = 200, loss = 0.0194\n",
      "t = 300, loss = 0.0104\n",
      "t = 400, loss = 0.0181\n",
      "t = 500, loss = 0.0593\n",
      "t = 600, loss = 0.0145\n",
      "t = 700, loss = 0.0400\n",
      "Starting epoch 10 / 10\n",
      "t = 100, loss = 0.0036\n",
      "t = 200, loss = 0.0975\n",
      "t = 300, loss = 0.0275\n",
      "t = 400, loss = 0.0134\n",
      "t = 500, loss = 0.0311\n",
      "t = 600, loss = 0.0386\n",
      "t = 700, loss = 0.0163\n",
      "Checking accuracy on validation set\n",
      "Got 47900 / 49000 correct (97.76)\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "Wall time: 25min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "torch.random.manual_seed(12345)\n",
    "model.apply(reset)\n",
    "train(model, loss_fn, optimizer, num_epochs=10)\n",
    "check_accuracy(model, loader_train)\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "t = 100, loss = 1.3932\n",
      "t = 200, loss = 1.5089\n",
      "t = 300, loss = 1.4553\n",
      "t = 400, loss = 1.0639\n",
      "t = 500, loss = 0.9470\n",
      "t = 600, loss = 1.2640\n",
      "t = 700, loss = 1.3742\n",
      "Starting epoch 2 / 10\n",
      "t = 100, loss = 0.8434\n",
      "t = 200, loss = 0.9628\n",
      "t = 300, loss = 1.0083\n",
      "t = 400, loss = 0.7999\n",
      "t = 500, loss = 0.7557\n",
      "t = 600, loss = 0.9484\n",
      "t = 700, loss = 0.9602\n",
      "Starting epoch 3 / 10\n",
      "t = 100, loss = 0.6545\n",
      "t = 200, loss = 0.7131\n",
      "t = 300, loss = 0.7258\n",
      "t = 400, loss = 0.8530\n",
      "t = 500, loss = 0.5801\n",
      "t = 600, loss = 0.7880\n",
      "t = 700, loss = 0.6514\n",
      "Starting epoch 4 / 10\n",
      "t = 100, loss = 0.4065\n",
      "t = 200, loss = 0.5517\n",
      "t = 300, loss = 0.4938\n",
      "t = 400, loss = 0.5849\n",
      "t = 500, loss = 0.5441\n",
      "t = 600, loss = 0.5378\n",
      "t = 700, loss = 0.4907\n",
      "Starting epoch 5 / 10\n",
      "t = 100, loss = 0.2361\n",
      "t = 200, loss = 0.4777\n",
      "t = 300, loss = 0.2568\n",
      "t = 400, loss = 0.4026\n",
      "t = 500, loss = 0.2929\n",
      "t = 600, loss = 0.2884\n",
      "t = 700, loss = 0.3534\n",
      "Starting epoch 6 / 10\n",
      "t = 100, loss = 0.2290\n",
      "t = 200, loss = 0.2317\n",
      "t = 300, loss = 0.2857\n",
      "t = 400, loss = 0.2770\n",
      "t = 500, loss = 0.1608\n",
      "t = 600, loss = 0.3177\n",
      "t = 700, loss = 0.1815\n",
      "Starting epoch 7 / 10\n",
      "t = 100, loss = 0.1252\n",
      "t = 200, loss = 0.3807\n",
      "t = 300, loss = 0.1913\n",
      "t = 400, loss = 0.1869\n",
      "t = 500, loss = 0.2182\n",
      "t = 600, loss = 0.1336\n",
      "t = 700, loss = 0.1365\n",
      "Starting epoch 8 / 10\n",
      "t = 100, loss = 0.1103\n",
      "t = 200, loss = 0.3083\n",
      "t = 300, loss = 0.2158\n",
      "t = 400, loss = 0.1291\n",
      "t = 500, loss = 0.1587\n",
      "t = 600, loss = 0.1563\n",
      "t = 700, loss = 0.2607\n",
      "Starting epoch 9 / 10\n",
      "t = 100, loss = 0.1817\n",
      "t = 200, loss = 0.1791\n",
      "t = 300, loss = 0.1596\n",
      "t = 400, loss = 0.1733\n",
      "t = 500, loss = 0.1834\n",
      "t = 600, loss = 0.0876\n",
      "t = 700, loss = 0.1169\n",
      "Starting epoch 10 / 10\n",
      "t = 100, loss = 0.1041\n",
      "t = 200, loss = 0.1057\n",
      "t = 300, loss = 0.2740\n",
      "t = 400, loss = 0.1087\n",
      "t = 500, loss = 0.1907\n",
      "t = 600, loss = 0.0820\n",
      "t = 700, loss = 0.1344\n",
      "Checking accuracy on validation set\n",
      "Got 48179 / 49000 correct (98.32)\n",
      "Checking accuracy on validation set\n",
      "Got 738 / 1000 correct (73.80)\n",
      "Wall time: 32min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "torch.random.manual_seed(12345)\n",
    "model.apply(reset)\n",
    "train(model, loss_fn, optimizer, num_epochs=10)\n",
    "check_accuracy(model, loader_train)\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model).  This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 7171 / 10000 correct (71.71)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
