{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses_epoch = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            losses_epoch.append(loss*actual_batch_size)\n",
    "            \n",
    "            '''\n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            '''          \n",
    "                \n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses_epoch)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Первоначальная модель\n",
    "# Feel free to play with this cell\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[7, 7, 3, 32])\n",
    "    b1 = tf.get_variable('b1', shape=[32])\n",
    "    W2 = tf.get_variable('W2', shape=[5408, 1024])\n",
    "    b2 = tf.get_variable('b2', shape=[1024])\n",
    "    W3 = tf.get_variable('W3', shape=[1024, 10])\n",
    "    b3 = tf.get_variable('b3', shape=[10])\n",
    "    \n",
    "    conv  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv)\n",
    "    bn = tf.layers.batch_normalization(relu1, training=is_training)\n",
    "    pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    pool_flat = tf.reshape(pool, [-1,5408])\n",
    "    af1 = tf.matmul(pool_flat, W2) + b2\n",
    "    relu2 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu2, W3) + b3\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 1e-2\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 conv layers\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 1024])\n",
    "    b3 = tf.get_variable('b3', shape=[1024])\n",
    "    W4 = tf.get_variable('W4', shape=[1024, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    #bn = tf.layers.batch_normalization(relu1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(pool1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    pool_flat = tf.reshape(pool2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 1e-2\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 conv layers + bn\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 1024])\n",
    "    b3 = tf.get_variable('b3', shape=[1024])\n",
    "    W4 = tf.get_variable('W4', shape=[1024, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 conv layers + bn + 2fc\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 1024])\n",
    "    b3 = tf.get_variable('b3', shape=[1024])\n",
    "    \n",
    "    W4 = tf.get_variable('W4', shape=[1024, 1024])\n",
    "    b4 = tf.get_variable('b4', shape=[1024])\n",
    "    W5 = tf.get_variable('W5', shape=[1024, 1024])\n",
    "    b5 = tf.get_variable('b5', shape=[1024])\n",
    "    W6 = tf.get_variable('W6', shape=[1024, 10])\n",
    "    b6 = tf.get_variable('b6', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    \n",
    "    relu4 = tf.nn.relu(af2)\n",
    "    af3 = tf.matmul(relu4, W5) + b5\n",
    "    relu5 = tf.nn.relu(af3)\n",
    "    af4 = tf.matmul(relu5, W6) + b6\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_out = af4\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 conv layers + bn + 1024->2048\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 7\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 6\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 2048])\n",
    "    b3 = tf.get_variable('b3', shape=[2048])\n",
    "    W4 = tf.get_variable('W4', shape=[2048, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 conv layers + bn + 1024->2048 + change H_f\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 3\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 32\n",
    "H_f2 = 2\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 2048])\n",
    "    b3 = tf.get_variable('b3', shape=[2048])\n",
    "    W4 = tf.get_variable('W4', shape=[2048, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 conv layers + bn + 1024->2048 + change H_f + change F\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "F1 = 32\n",
    "H_f1 = 3\n",
    "H_c1 = (32 - H_f1 + 1)/1       #26\n",
    "H_pool1 = ((H_c1 - 2)/2) + 1   #13\n",
    "w2_size0 = int(F1*H_pool1*H_pool1)   #5408\n",
    "\n",
    "F2 = 64\n",
    "H_f2 = 2\n",
    "pad2 = 1\n",
    "H_c2 = (H_pool1 - H_f2 + 1)/1  #8\n",
    "H_pool2 = ((H_c2 - 2)/2) + 1     #4\n",
    "w3_size0 = int(F2*H_pool2*H_pool2)     #512\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights\n",
    "    W1 = tf.get_variable('W1', shape=[H_f1, H_f1, 3, F1])\n",
    "    b1 = tf.get_variable('b1', shape=[F1])\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[H_f2, H_f2, 32, F2])\n",
    "    b2 = tf.get_variable('b2', shape=[F2])\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[w3_size0, 2048])\n",
    "    b3 = tf.get_variable('b3', shape=[2048])\n",
    "    W4 = tf.get_variable('W4', shape=[2048, 10])\n",
    "    b4 = tf.get_variable('b4', shape=[10])\n",
    "    \n",
    "    conv1  = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='VALID') + b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1  = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn1 = tf.layers.batch_normalization(pool1, training=is_training)\n",
    "    #pool  = tf.nn.max_pool(bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    conv2  = tf.nn.conv2d(bn1, W2, strides=[1,1,1,1], padding='VALID') + b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    bn2 = tf.layers.batch_normalization(pool2, training=is_training)\n",
    "    \n",
    "    pool_flat = tf.reshape(bn2, [-1,w3_size0])\n",
    "    af1 = tf.matmul(pool_flat, W3) + b3\n",
    "    relu3 = tf.nn.relu(af1)\n",
    "    af2 = tf.matmul(relu3, W4) + b4\n",
    "    y_out = af2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "lr = 5e-4\n",
    "#optimizer = tf.train.RMSPropOptimizer(lr) # select optimizer and set learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 1.23 and accuracy of 0.588\n",
      "Epoch 2, Overall loss = 0.662 and accuracy of 0.777\n",
      "Epoch 3, Overall loss = 0.382 and accuracy of 0.88\n",
      "Epoch 4, Overall loss = 0.225 and accuracy of 0.93\n",
      "Epoch 5, Overall loss = 0.178 and accuracy of 0.941\n",
      "Epoch 6, Overall loss = 0.127 and accuracy of 0.959\n",
      "Epoch 7, Overall loss = 0.0831 and accuracy of 0.974\n",
      "Epoch 8, Overall loss = 0.0703 and accuracy of 0.977\n",
      "Epoch 9, Overall loss = 0.0643 and accuracy of 0.979\n",
      "Epoch 10, Overall loss = 0.0615 and accuracy of 0.979\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2901079e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 0.0535 and accuracy of 0.983\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.75 and accuracy of 0.698\n",
      "Wall time: 27min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,10,64,100,train_step,True)\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на трейне и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 0.0535 and accuracy of 0.983\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.75 and accuracy of 0.698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.7525476703643799, 0.698)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 70% accuracy on Validation\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test Set - Do this only once\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Epoch 1, Overall loss = 1.72 and accuracy of 0.704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.7242294297218324, 0.704)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
